from enum import Enum

from scipy.spatial.distance import cdist
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from collections import Counter
import pyprind
import sys


class DistType(Enum):
    Euclidian = "euclidean"
    OneClass = "one_class"
    

class DistanceCalculator:
    def __init__(self, batch_size=1):
        self._batch_size = batch_size

    def euclidean(self, mx1, mx2, typ="euclidean"):
        # Get euclidean distances as 2D array
        dists = cdist(mx1, mx2, typ)
        # return the most distant rows
        # mx1 - test matrix
        # mx2 - train matrix
        # index of max value in dists (r, c)
        # TODO change to average distance ?
        # TODO RETURN K
        # TODO Think about first reveal
        top_index = dists.mean(axis=1).argsort(kind='heapsort')[0:self._batch_size]         
        return top_index.tolist()

    # TODO:implement using one class svm- http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
    def one_class_learning(self, x_train, x_test):
        model = svm.OneClassSVM()
        model.fit(x_train)
        pred = model.predict(x_test)
        # -1 is abnormal group
        indices = np.where(pred == -1)[0]
        if len(indices) >= 1:
            random_idx = np.random.randint(0, len(indices))
            # TODO RETURN K
            return indices[random_idx]
        else:
            return np.random.randint(0, len(x_test))


class Learning:
    def __init__(self, batch_size=1):
        self._batch_size = batch_size
        
    def machine_learning(self, x_train, y_train, x_test, smallest_class, clf=None):
        # smallest class -> 1/0
        if clf is None:
            # n_estimators - number of trees
            # balances -
            clf = RandomForestClassifier(n_estimators=200, class_weight="balanced")
        clf.fit(np.asmatrix(x_train, dtype=np.float32), y_train)
        probs = clf.predict_proba(x_test)
        # probs closer to 0 -> 0
        # sorting high to low hence the minus sign
        return (-probs[:, smallest_class]).argsort(kind='heapsort')[0:self._batch_size].tolist()  # smallest class black

    # TODO: implementation using keras
    def deep_learning(self, x_train, y_train, x_test, smallest_class, keras=None):
        # pass
        from keras import Sequential
        from keras.callbacks import EarlyStopping
        from keras.layers import Dense, Dropout
        from keras.regularizers import l1_l2
        # stop if there is no improvement
        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1)
        self.classifier = Sequential()
        # he_normal - init weights normal
        self.classifier.add(Dense(300, kernel_initializer="he_normal", activation="relu", input_dim=x_train.shape[1]))
        self.classifier.add(Dropout(0.5))
        self.classifier.add(Dense(100, kernel_initializer='he_normal', activation='relu', kernel_regularizer=l1_l2(0.5)))
        self.classifier.add(Dropout(0.5))
        self.classifier.add(Dense(20, kernel_initializer='he_normal', activation='relu', kernel_regularizer=l1_l2(0.5)))
        self.classifier.add(Dropout(0.5))
        self.classifier.add(Dense(1, kernel_initializer='uniform', activation="sigmoid", kernel_regularizer=l1_l2(0.1)))

        self.classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        self.classifier.fit(x_train, y_train, validation_split=0.1, callbacks=[early_stopping], epochs=10,
                            batch_size=520, verbose=0)
        probs = self.classifier.predict_proba(x_test)
        # sorting high to low hence the minus sign
        return (-probs).argsort(kind='heapsort')[0:self._batch_size].tolist()


class ExploreExploit:
    def __init__(self, tags_vect, features_mx, recall, eps, batch_size=1):
        self._batch_size = batch_size
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1][0]  # who is the black
        self._n_black = Counter(tags_vect).most_common()[-1][1]  # number of blacks
        self._stop_cond = np.round(recall * self._n_black)  # number of blacks to find - stop condition
        self._eps = eps
        self._time = 0          # how many nodes we asked about
        self._num_black_found = 0
        self._num_nodes = len(tags_vect)    # number of all nodes

    def _init(self):
        # initialize the train objects with copies - COPY
        # self.x_test = type(self._features_mx)(self._features_mx)
        self.x_test = self._features_mx.copy()
        self.y_test = list(self._tags)
        self.x_train = None
        self.y_train = None
        self.bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)  # optional
        # explore first using distance
        start_k = 2 if self._batch_size == 1 else 1
        for i in range(start_k):
            # first two nodes (index)
            top_index = DistanceCalculator(batch_size=self._batch_size).euclidean(self.x_test, self.x_test)  
            self._reveal(top_index)

    def run(self, dist_calc_type):
        self._num_black_found = 0
        self._init()
        # while not 0.7 recall
        while self._num_black_found < self._stop_cond:
            rand = np.random.uniform(0, 1)
            # 0 < a < eps -> distance based  || at least one black and white reviled -> one_class/ euclidean
            if rand < self._eps or len(Counter(self.y_train)) < 2:
                # idx -> most far away node index
                if dist_calc_type.value == DistType.Euclidian.value:
                    top_index = DistanceCalculator(batch_size=self._batch_size).euclidean(self.x_test, self.x_train)
                else:
                    top_index = DistanceCalculator(batch_size=self._batch_size).one_class_learning(self.x_train,
                                                                                                   self.x_test)
            else:
                # idx -> by learning
                top_index = Learning(batch_size=self._batch_size).machine_learning(self.x_train, self.y_train,
                                                                                   self.x_test, self._smallest_class)
            self._reveal(top_index)
            # number of guesses/ number of nodes .. , for figure
        return self._time/self._num_nodes, self.y_train

    def _reveal(self, top_index):
        self._time += 1
        self.bar.update()  # display
        # if self._time % 100 == 0:
        #     print(str(self._time) + " nodes were explored, time:" + str(datetime.now().time()))
        for idx in top_index:
            if self.y_test[idx] == self._smallest_class:
                self._num_black_found += 1
    
            # add feature vec to train
            if self.x_train is None:
                self.x_train = self.x_test[idx, :]
                self.y_train = []
            else:
                self.x_train = np.vstack([self.x_train, self.x_test[idx, :]])
            # y train
            self.y_train.append(self.y_test.pop(idx))

        for idx in top_index:
            # delete feature vec from test
            self.x_test = np.delete(self.x_test, idx, 0)
